<!DOCTYPE html>
<html>
<head>
    <title>ZC' bolg for test</title>
    <meta charset="utf-8">

    <!-- 引入配置文件 -->
    
<link rel="stylesheet" href="/css/main.css">


    <!-- 字体图片库 -->
    
<link rel="stylesheet" href="/lib/font-awesome-4.7.0/css/font-awesome.min.css">


    <!-- 代码高亮库 -->
    
<link rel="stylesheet" href="/lib/highlight/styles/atom-one-dark.css">

    
<meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="YYHY's Blog" type="application/atom+xml">
</head>
<body>

    <div id="main">
        <!-- 引入侧边栏 -->
        <aside id="#aside">
            <!-- 搜索栏 -->
<div id="search">
    <input class="search-input" type="text" placeholder="search">
    <i id="search-icon" class="fa fa-bars" title="切换目录与索引">
</div>


<!-- 侧边目录栏 -->
<div id="tree">
    

    
                    <ul>
                        <li class="file">
                            <a href="/2023/07/03/Front-matter示例/">
                                <i class="fa fa-file"></i>
                                Front-matter示例
                            </a>
                        </li>
                    </ul>
    
                    <ul>
                        <li class="file">
                            <a href="/2023/07/20/SVM/">
                                <i class="fa fa-file"></i>
                                SVM
                            </a>
                        </li>
                    </ul>
    
                    <ul>
                        <li class="file">
                            <a href="/2024/08/13/使用Unity创建一个类似MC的游戏场景（一）/">
                                <i class="fa fa-file"></i>
                                使用Unity创建一个类似MC的游戏场景（一）
                            </a>
                        </li>
                    </ul>
    
                    <ul>
                        <li class="file">
                            <a href="/2023/07/22/决策树/">
                                <i class="fa fa-file"></i>
                                决策树
                            </a>
                        </li>
                    </ul>
    
                    <ul>
                        <li class="file">
                            <a href="/2023/07/25/图像表示和图像增强（一）/">
                                <i class="fa fa-file"></i>
                                图像表示和图像增强（一）
                            </a>
                        </li>
                    </ul>
    
                    <ul>
                        <li class="file">
                            <a href="/2023/07/29/图像表示和图像增强（二）/">
                                <i class="fa fa-file"></i>
                                图像表示和图像增强（二）
                            </a>
                        </li>
                    </ul>
    
                    <ul>
                        <li class="file">
                            <a href="/2023/06/30/数据结构课程实习/">
                                <i class="fa fa-file"></i>
                                数据结构课程实习
                            </a>
                        </li>
                    </ul>
    
                    <ul>
                        <li class="file">
                            <a href="/2023/07/16/线性回归/">
                                <i class="fa fa-file"></i>
                                线性回归
                            </a>
                        </li>
                    </ul>
    
                    <ul>
                        <li class="file active">
                            <a href="/2023/07/18/逻辑回归/">
                                <i class="fa fa-file"></i>
                                逻辑回归
                            </a>
                        </li>
                    </ul>
    
</div>

<!-- 最尾部添加，这里就不列以前的代码了 -->
<div id="toc" style="display: none;"></div>

        </aside>

        <!-- 引入导航 -->
        <nav>
            <ul id="menu">
    <!-- 内部链接本页面直接跳转 -->
    
    <li class="menu-item">
        <a href="/ || fas fa-home" class="menu-item-link">首页</a>
    </li>
    
    <li class="menu-item">
        <a href="/archives/ || fas fa-archive" class="menu-item-link">归档</a>
    </li>
    
    <li class="menu-item">
        <a href="/tags/ || fas fa-tags" class="menu-item-link">标签</a>
    </li>
    
    <li class="menu-item">
        <a href="/categories/ || fas fa-folder-open" class="menu-item-link">分类</a>
    </li>
    
    <li class="menu-item">
        <a href="/about/ || fas fa-heart" class="menu-item-link">关于</a>
    </li>
    

    <!-- 外部链接打开新的窗口跳转 -->
    
    <li class="menu-item">
        <a href="https://www.cnblogs.com/yyhh/p/11058985.html" class="menu-item-link" target="_blank">参考教程</a>
    </li>
    
    <li class="menu-item">
        <a href="https://github.com/youyouhangyong" class="menu-item-link" target="_blank">github</a>
    </li>
    

</ul>

        </nav>

        <!-- 引入正文 -->
        <div id="content">
      <div>
    <span id="post-author">作者: YYHY</span>
    <span id="post-date">2023-07-18 13:37:28</span>
</div>

<div id="article">
    <p>逻辑回归本质上是一个<strong>分类模型</strong>,用于预测输入数据属于哪个类别的<strong>概率</strong>。其基本思想是:</p>
<ol type="1">
<li>构建一个线性函数,输入是样本特征,输出是样例属于某类的对数概率。</li>
<li>将线性函数的输出通过sigmoid函数转换为概率值。</li>
<li>根据最大概率,对样本进行分类预测。</li>
<li>通过最大化似然估计或最小化交叉熵损失,训练线性模型的参数。</li>
</ol>
<p>具体实现原理:</p>
<p>假设逻辑回归为<strong>二分类</strong>,线性函数为:</p>
<p><span class="math display">\[z = w_1 x_1 + w_2 x_2 + ... + w_n x_n +
b\]</span></p>
<p>其中:</p>
<ul>
<li><span class="math inline">\(z\)</span> - 模型的预测值</li>
<li><span class="math inline">\(x_1, x_2,...,x_n\)</span> - <span
class="math inline">\(n\)</span>个特征变量</li>
<li><span class="math inline">\(w_1, w_2,...,w_n\)</span> - <span
class="math inline">\(n\)</span>个特征变量对应的模型参数权重</li>
<li><span class="math inline">\(b\)</span> - 模型的偏置项</li>
</ul>
<p>将z代入sigmoid函数求概率:</p>
<p><span class="math display">\[p = \frac{1}{1+e^{-z}}\]</span></p>
<p>其中: p表示输入样本为正类的概率。</p>
<p>则逻辑回归的损失函数为负对数似然或交叉熵。</p>
<p>通过梯度下降算法训练w和b,最小化损失,完成模型。</p>
<p>预测时,计算新样本的概率p,如果p&gt;0.5则预测为正类,否则为负类。</p>
<p>在逻辑回归模型中,常用的损失函数有以下两种:</p>
<ol type="1">
<li>对数损失(Log
Loss):也称对数似然损失函数,定义为负的对数似然函数,表达式为:</li>
</ol>
<p><span class="math display">\[Loss = -\frac{1}{N}\sum_{i=1}^N\left[
y_i\log(p_i) + (1-y_i)\log(1-p_i) \right]\]</span></p>
<p>其中:- <span class="math inline">\(N\)</span> - 样本数量</p>
<ul>
<li><span class="math inline">\(y_i\)</span> - 第<span
class="math inline">\(i\)</span>个样本的真实标签,取值为0或1</li>
<li><span class="math inline">\(p_i\)</span> - 第<span
class="math inline">\(i\)</span>个样本的预测概率。求和是对所有样本进行遍历。</li>
</ul>
<ol type="1">
<li>交叉熵损失(Cross
Entropy):它扩展了对数损失到多分类问题,表达式为:</li>
</ol>
<p><span class="math display">\[Loss = -\frac{1}{N} \sum_{i=1}^{N}
y_{i}\log(p_{i})\]</span></p>
<p>其中:- <span class="math inline">\(N\)</span> - 样本数量 - <span
class="math inline">\(y_i\)</span> - 第<span
class="math inline">\(i\)</span>个样本的one-hot编码标签向量 - <span
class="math inline">\(p_i\)</span> - 第<span
class="math inline">\(i\)</span>个样本的预测概率向量</p>
<p><span class="math inline">\(y_i\)</span>
是one-hot向量,只有对应真实类别的单个元素值为1。<span
class="math inline">\(p_i\)</span> 是预测概率向量。</p>
<p><span class="math inline">\(y_i\log(p_i)\)</span>
计算两个概率分布差异的交叉熵。</p>
<p>求和后取负号得到交叉熵损失。</p>
<p><strong>Sigmoid函数的介绍：</strong></p>
<p>Sigmoid函数将任意实数映射到(0, 1)区间内,表达为上述分数形式。</p>
<p>当输入<span class="math inline">\(z\)</span>趋近正无穷大时,<span
class="math inline">\(e^{-z}\)</span>趋近0,分母趋近1,所以<span
class="math inline">\(p\)</span>趋近1。</p>
<p>当输入<span class="math inline">\(z\)</span>趋近负无穷大时,<span
class="math inline">\(e^{-z}\)</span>趋近无穷大,分母趋近无穷大,所以<span
class="math inline">\(p\)</span>趋近0。</p>
<p>sigmoid函数的这个S形曲线图形,利用它可以将任意值映射到概率之间,所以常被用作神经网络中门限函数和逻辑回归中概率输出函数。</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pCTVfNd"><img
src="https://s1.ax1x.com/2023/07/18/pCTVfNd.png"
alt="pCTVfNd.png" /></a></p>
<p>抽象的说：逻辑回归=线性回归+Sigmoid函数</p>
<p>构造两个聚类式的高斯分布数据,用于逻辑回归分类:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>mean1 <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>mean2 <span class="op">=</span> [<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>cov <span class="op">=</span> [[<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>]] </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>X1 <span class="op">=</span> np.random.multivariate_normal(mean1, cov, <span class="dv">100</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>X2 <span class="op">=</span> np.random.multivariate_normal(mean2, cov, <span class="dv">100</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.concatenate((X1, X2)) </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">0</span>]<span class="op">*</span><span class="dv">100</span> <span class="op">+</span> [<span class="dv">1</span>]<span class="op">*</span><span class="dv">100</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 训练和预测</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> LogisticRegression()</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>clf.fit(X, y) </span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 画出数据分布和决策边界</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:,<span class="dv">0</span>], X[:,<span class="dv">1</span>], c<span class="op">=</span>y)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> plt.gca()  </span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>xlim <span class="op">=</span> ax.get_xlim()</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>ylim <span class="op">=</span> ax.get_ylim()</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>xx <span class="op">=</span> np.linspace(xlim[<span class="dv">0</span>], xlim[<span class="dv">1</span>], <span class="dv">30</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>yy <span class="op">=</span> np.linspace(ylim[<span class="dv">0</span>], ylim[<span class="dv">1</span>], <span class="dv">30</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>YY, XX <span class="op">=</span> np.meshgrid(yy, xx)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>xy <span class="op">=</span> np.vstack([XX.ravel(), YY.ravel()]).T</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> clf.decision_function(xy).reshape(XX.shape)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>ax.contour(XX, YY, Z, colors<span class="op">=</span><span class="st">&#39;k&#39;</span>, levels<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p>这里构造了两个高斯分布,<strong>分别围绕(1,1)和(-1,-1)生成类0和类1的数据</strong>。可以看到逻辑回归学习到了一个合适的决策边界将两个类别尽量分开。</p>
<p><a target="_blank" rel="noopener" href="https://imgse.com/i/pCTZXqO"><img
src="https://s1.ax1x.com/2023/07/18/pCTZXqO.png"
alt="pCTZXqO.png" /></a></p>

</div>

        </div>
        
    </div>

    <!-- 引入 js 文件 -->
    
<script src="/js/main.js"></script>


    <!-- 引入代码高亮的 js -->
    
<script src="/lib/highlight/highlight.min.js"></script>

    <script>hljs.initHighlightingOnLoad();</script>

    <!-- 引入 jquery -->
    
<script src="/lib/jquery-3.5.1.min.js"></script>

    <script>hljs.initHighlightingOnLoad();</script>

    <!-- 引入 pjax -->
    
<script src="/lib/jquery.pjax.js"></script>



    <script src="https://blog-static.cnblogs.com/files/yyhh/L2Dwidget.min.js"></script>

    <script type="text/javascript">
    L2Dwidget.init();
</script>

</body>
</html>
                                                                                                     